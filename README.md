# OnLLM
OnLLM is the platform to run LLM or SLM models using OnnxRuntime directly on low-end devices like low power computers, mobile phones etc. It is cross-platform using Kivy &amp; open-source
